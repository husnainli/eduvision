import streamlit as st
import fitz  # PyMuPDF
import re
import hashlib
from utils.embeddings import chunk_text, embed_chunks, retrieve_from_all_vectorstores, find_most_similar_summary
from utils.llm import query_llama3, summarize_text_arabic
from utils.translate import translate_text

# -------------------------------
# ğŸ§¼ Arabic Text Cleaning Utility
# -------------------------------
def clean_arabic_text(text):
    """Cleans and normalizes Arabic text."""
    diacritics = re.compile(r"[Ù‹ÙŒÙÙÙÙÙ‘Ù’Ù“]")
    text = re.sub(diacritics, '', text)
    text = re.sub(r"[Ø¥Ø£Ø¢Ø§]", "Ø§", text)
    text = re.sub(r"Ù‰", "ÙŠ", text)
    text = re.sub(r"Ø¤", "Ùˆ", text)
    text = re.sub(r"Ø¦", "ÙŠ", text)
    text = re.sub(r"Ø©", "Ù‡", text)
    text = re.sub(r"[^\u0600-\u06FF\s]", '', text)
    return re.sub(r"\s+", ' ', text).strip()

# -------------------------------
# ğŸ§  Hashing for Caching
# -------------------------------
def compute_text_hash(text):
    return hashlib.md5(text.encode('utf-8')).hexdigest()

@st.cache_resource
def get_cached_vectorstore(chunks, filename, text_hash):
    return embed_chunks(chunks, filename=filename)

@st.cache_data(show_spinner=False)
def get_cached_summary(text, filename, text_hash):
    short_text = text[:3000]
    return summarize_text_arabic(short_text)

# -------------------------------
# ğŸ“„ PDF Text Extraction
# -------------------------------
def extract_text_from_pdf(pdf_file):
    """Extracts and cleans Arabic text from uploaded PDF."""
    doc = fitz.open(stream=pdf_file.read(), filetype="pdf")
    raw_text = ''.join(page.get_text() for page in doc)
    return clean_arabic_text(raw_text)

# -------------------------------
# ğŸš€ Streamlit App Initialization
# -------------------------------
st.set_page_config(page_title="ğŸ“š EduVision AI", layout="wide")
st.title("ğŸ¤– EduVision AI")

# Initialize chat history
if "messages" not in st.session_state:
    st.session_state.messages = []

# -------------------------------
# ğŸ“¤ PDF Upload
# -------------------------------
uploaded_files = st.file_uploader("ğŸ“¤ Upload Arabic PDFs", type=["pdf"], accept_multiple_files=True)

vectorstores = []

if uploaded_files:
    summaries = []

    for pdf_file in uploaded_files:
        filename = pdf_file.name
        st.success(f"âœ… {filename} uploaded successfully!")

        # ğŸ” Extract and clean text
        with st.spinner(f"ğŸ§¼ Extracting and cleaning text from {filename}..."):
            pdf_text = extract_text_from_pdf(pdf_file)

        # ğŸ”„ Split into chunks
        with st.spinner(f"ğŸ”„ Splitting {filename} into chunks..."):
            chunks = chunk_text(pdf_text)

        # ğŸ§  Hash PDF text to cache vector store
        text_hash = compute_text_hash(pdf_text)

        # ğŸ§  Generate and cache embeddings
        with st.spinner(f"ğŸ§  Embedding text from {filename}..."):
            vs = get_cached_vectorstore(chunks, filename, text_hash)
            vectorstores.append((filename, vs))

        # ğŸ“ Generate summary of full text
        with st.spinner(f"ğŸ“š Generating Arabic summary for {filename}..."):
            summary = get_cached_summary(pdf_text, filename, text_hash)
            summaries.append((filename, summary))

        with st.container():
            st.markdown(f"### ğŸ“ Ù…Ù„Ø®Øµ Ø§Ù„ÙˆØ«ÙŠÙ‚Ø©: {filename}")
            st.markdown(
                f"""
                <div style='background-color:#f9f9f9;
                            border-left: 5px solid #4CAF50;
                            padding: 1rem;
                            border-radius: 10px;
                            font-size: 1.1rem;
                            direction: rtl;
                            text-align: right;'>
                    {summary}
                </div>
                """,
                unsafe_allow_html=True
            )

    # ---------------------------------
    # ğŸ’¬ Interactive Q&A Chat Interface
    # ---------------------------------
    st.divider()
    st.subheader("ğŸ’¬ Ask a question based on the uploaded PDFs")

    for msg in st.session_state.messages:
        with st.chat_message(msg["role"]):
            st.markdown(msg["content"])

    user_input = st.chat_input("âœï¸ Ø§ÙƒØªØ¨ Ø³Ø¤Ø§Ù„Ùƒ Ù‡Ù†Ø§ (Ø¨Ø§Ù„Ù„ØºØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©)...")

    if user_input:
        # Show user question
        st.session_state.messages.append({"role": "user", "content": user_input})
        with st.chat_message("user"):
            st.markdown(user_input)

        # Retrieve relevant document chunks
        with st.spinner("ğŸ¤– Generating response using LLaMA 3..."):
            retrieved_docs = retrieve_from_all_vectorstores(vectorstores, user_input, k_per_doc=4)

            context = "\n\n".join(
                f"[Ù…Ù† Ø§Ù„Ù…Ù„Ù: {doc.metadata.get('source', 'ØºÙŠØ± Ù…Ø¹Ø±ÙˆÙ')}]\n{doc.page_content}"
                for doc in retrieved_docs
            )

            prompt = (
                f"Ø§Ù„Ø³Ø¤Ø§Ù„:\n{user_input}\n\n"
                f"Ù…Ø­ØªÙˆÙ‰ Ø§Ù„ÙˆØ«Ø§Ø¦Ù‚:\n{context}\n\n"
                "Ø£Ø¬Ø¨ Ø¥Ø¬Ø§Ø¨Ø© ÙƒØ§Ù…Ù„Ø© ÙˆØ´Ø§Ù…Ù„Ø© Ù…Ø³ØªÙ†Ø¯Ù‹Ø§ ÙÙ‚Ø· Ø¥Ù„Ù‰ Ù…Ø­ØªÙˆÙ‰ Ø§Ù„ÙˆØ«Ø§Ø¦Ù‚ Ø£Ø¹Ù„Ø§Ù‡.\n"
                "ÙŠØ¬Ø¨ Ø£Ù† ØªÙƒÙˆÙ† Ø¥Ø¬Ø§Ø¨ØªÙƒ Ø¨Ø§Ù„Ù„ØºØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ø§Ù„ÙØµØ­Ù‰ ÙÙ‚Ø· Ø¯ÙˆÙ† Ø£ÙŠ ÙƒÙ„Ù…Ø§Øª Ø¥Ù†Ø¬Ù„ÙŠØ²ÙŠØ© Ø£Ùˆ Ù„ØºØ§Øª Ø£Ø®Ø±Ù‰.\n"
                "Ø¥Ø°Ø§ Ù„Ù… ÙŠÙƒÙ† Ù‡Ù†Ø§Ùƒ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª ÙƒØ§ÙÙŠØ©ØŒ Ù‚Ù„ Ø°Ù„Ùƒ Ø¨Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© ÙÙ‚Ø· Ø¯ÙˆÙ† ØªØ£Ù„ÙŠÙ."
            )


            response = query_llama3(prompt)

            # # âœ… Extract unique source filenames
            # used_sources = set(doc.metadata.get("source", "ØºÙŠØ± Ù…Ø¹Ø±ÙˆÙ") for doc in retrieved_docs)
            # sources_line = "ğŸ—‚ï¸ Ø§Ù„Ù…Ø±Ø§Ø¬Ø¹: [" + "ØŒ ".join(used_sources) + "]"

            # from collections import Counter
            # source_counter = Counter(doc.metadata.get("source", "ØºÙŠØ± Ù…Ø¹Ø±ÙˆÙ") for doc in retrieved_docs)
            # top_sources = [src for src, count in source_counter.most_common(1)]
            # sources_line = "ğŸ—‚ï¸ Ø§Ù„Ù…Ø±Ø§Ø¬Ø¹: [" + "ØŒ ".join(top_sources) + "]"
            
            # âœ… NEW: Match response to the most similar summary
            top_source = find_most_similar_summary(response, summaries)
            sources_line = f"ğŸ—‚ï¸ Ø§Ù„Ù…Ø±Ø¬Ø¹: [{top_source}]"
            # âœ… Append source references to the final message
            final_response = f"{response.strip()}\n\n{sources_line}"

        # âœ… Show the response
        with st.chat_message("assistant"):
            st.markdown(final_response)

        with st.spinner("Translating to English..."):
            translation_result = translate_text(final_response)
            st.markdown(f"ğŸ“— English Translation: `{translation_result}`")

        # âœ… Add to chat history
        st.session_state.messages.append({"role": "assistant", "content": final_response})

else:
    st.info("â¬†ï¸ Ø§Ù„Ø±Ø¬Ø§Ø¡ Ø±ÙØ¹ Ù…Ù„Ù PDF Ù„Ø¨Ø¯Ø¡ Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø©.")
